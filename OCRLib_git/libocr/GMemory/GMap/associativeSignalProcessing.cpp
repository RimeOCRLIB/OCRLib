#include "GMap.h"

using namespace std;
using namespace ocr;


void GMap::associativeSignalProcessing(string&text){ 

    // ассоциативная обработка сигналов // 
    int print=0;
    DR(endl<<"*****************************************************************************************************"<<endl)
    DR(endl<<"Start program *associativeSignalProcessing*  *старт пакета функций ассоциативной обработки сигналов*"<<endl)
    
    
    // описание переннести в хелп //
    //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////// 
    
    // Литература // Описание вынести в отдельный файл help
    
    // Кибернетика, или Управление и связь в животном и машине. 
    // Винер Н.  Пер. с англ. И.В. Соловьева и Г.Н. Поварова; Под ред. Г.Н. Поварова. – 2-е издание. – М.: Наука;
    // Cybernetics or Control and Communication in the Animal and the Machine. Norbert Wiener.    
    // Редакция кибернетической литературы, 1968. – 328 с.
    // Глава VI. Гештальд и универсалии. стр 202.
    
    // Теория;
    // Радиотехнические цепи и сигналы. Радио и связь Москва, 1986. Гоноровский Иосиф Семенович. 
    // Radio Technical Circuits and Signals. Radio i sviaz, Moscow, 1986. Gonorovskij Iosif Semenovich
    // 12.13. Алгоритм цифроврй фильтрации во временной и частотной областях. Стр 385.
    // 13.3. Импульсная характеристика согласованного фильтра. Стр 402.        
    
    // Реализация;
    // Алгоритмические трюки для программистов. Издательский дом "Вильяме" Москва * Санкт-Петербург * Киев 2004. Генри Уоррен, мл.
    // Hacker's DelightHenry.  S. Warren Jr.
    // Листинг 5.1. Подсчет количества единичных битов в слове. Стр 76.
    
    //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
    
    //  Корреляционный поиск. Поиск всех вхождений фразы в тексте. Реализация Марковских процессов. MarkovProcesses  //
    //  Общая идеология
    
    //  Э. Кнут "Искусство программирования" 2005г. 2е издание Т3 стр 611
    //  В версии pdf, распространенной в сети, тоже 2005г 2е издание Т3 стр 350
    //  Malcolm C. Harrison [CACM 14 (1971) 777-779]
    /*
     Это то что пишет Э. Кнут
     Малькольм Ч. Харрисон [CACM, 14 (1971), 777–779] заметил, что кодирование наложением можно использовать для ускорения поиска текста. 
     Предположим, что нам нужно определить все вхождения некоторой цепочки символов (словарной фразы) в большой текст, 
     без построения громоздкого дерева Патриции. 
     Предположим, кроме того, что текст поделен на строки c1 c2 . . . c50 по 50 символов в каждой. 
     Харрисон предлагает кодировать каждую из 49 пар символов c1 c2, c2 c3, . . . , c49 c50 путем отображения ее в число от 0 до, скажем, 127. 
     Затем подсчитать ”ключ” строки c1 c2 . . . c50 — цепочку из 128 битов b0 b1 . . . b127, где bi=1 тогда и только тогда, 
     когда h(cj,cj+1)=i при некотором j (т.е. заполнить единицами битовый массив размером [128,0]). Выборка пар символов происходит внахлест,
     а не встык. 
     Если нам нужно найти все вхождения слова ИГОЛКА (словарной фразы) в большой текстовой файл СТОГСЕНА, мы просто отыскиваем все строки, 
     ключи (массив) которых содержат 1 в позициях h(ИГ), h(ГО), h(ОЛ), h(ЛК), h(KA). При случайной хеш-функции вероятность того, 
     что некоторая случайная строка содержит в ключе (массиве) все эти единицы, равна всего лишь 0.00341 (ср. с упр. 4); 
     следовательно, пересечение пяти инвертированных списков цепочек битов позволит быстро найти все строки, содержащие слово ИГОЛКА 
     (хотя, вероятно, будет и несколько ложных выпадений).
     
    
     */ 
        
     //----------------------------------------------------------------------------------------------------------------------------------------
    
     /*
     Адаптация этого алгоритма к OCR для быстрого, корреляционного поиска с учетом под вхождений.
     Вместо хеша используется плотная паковка кодов (short) проверяемого текста и словаря т.е. простые таблицы. Каждая буква 
     проверяемого текста и словаря (одна бука один short) обладающая разреженными кодами в пространстве от 0 до 65535 кодируется 
     в пространство от 1 до 256 (примерно от 256 до 512), в зависимости от количества разных букв в словаре. Затем каждая пара букв 
     кодируется в пространство от 1 до 2048-8192 (примерно от 2048 до 8192), в зависимости от количеств разных пар букв в словаре. 
     В итоге каждая пара букв текста и словаря кодируется в пространство плотно упакованных кодов (подряд идущих кодов) одним short.
     Причем для паковки проверяемого текста используются таблицы (массивы) паковки словаря. Кодовое простраство словаря (700мб txt) 
     включает в себя все кодовое простраство текста.
      Функция table_Dict.h осуществляет создание таблиц (массивов) для плотной паковки или распаковки кодов букв и пар букв  
      и плотную паковку кодов словаря. 
      Функция table_Text.h осуществляет плотную паковку кодов букв и кодов пар букв проверяемого текста используя таблицы (массивы) 
      паковки словаря. 
      Функция table_Processing.h осуществляет Корреляционный поиск (подробно описан ниже)
      
      
     Пары букв. Простой вариант. Точно как у Э. Кнута
     В массиве options_text находится распознаннай текст. Читаем из массива text_data пары букв подсчитываем их плотно упакованный код пар букв 
     (short, это как 16 разрядный хеш). И используя вычисленный код в качестве адреса, заносим в массив ключей key_text (реальное имя BufUpT) 
     "единицу" ( можно и исходный код первой буквы из пары, пригодится потом ). 
     Далее берем следующею пару букв со двигом на единицу (одну букву) подсчитываем их код, заносим в массив ключей key_text "единицу" и т.д. 
     пока не пробежим парами букв весь массив text_data.
     Смещение пар букв на единицу (одну букву) необходимо для линейной фиксации взаимного положения букв относительно друг друга.
     После просмотра всего распознанного текста массив ключей оказывается немного заполнен адресами всех пар букв присутствующих в распознанном тексте.
     Теперь можно применять словарь. Словарь состоит из нумерованных и отделенных друг от друга переводами каретки (Enter) фраз. Каждая фраза 
     состоит из short (16р) с плотно упакованным кодом пар букв, смещенных на одну букву (как и в случае распознанного текста). 
     Хороший словарь это миллионы фраз (7,5 миллионов фраз, примерно по 20 букв на фразу, всего 700мб txt) и потенциально очень много много под вхождений.
     
     Процес корреляционного поиска состоит из двух этапов
     
     - Первый этап.
     Берем первую (по порядку словаря) словарную фразу и используя плотно упакованный коды пары букв, как адрес массива ключей key_text, 
     извлекаем из этого массива адреса Тибетских слогов распознанного текста и заносим их по порядку, как восстановленный текст в массив 
     скорректированного текста proof_reading_text.
     Долее берем следующею по порядку словаря словарную фразу и аналогично пополняем массив скоректированного текста и т.д. 
     В массив скоректированного текста попадают максимально правильные части распознанного текста. 
     Для каждой пары букв в массиве скоректированного текста рядом с ним пишется номер фразы которая его нашла в массиве ключей и ее вес.
     Во время прогона всего словаря, в отдельном массиве weight_dictionary_phrases пишется вес словарной фразы.
     Вес словарной фразы это сколько своих слогов она смогла найти в распознанном тексте. Эти данные понадобяться на втором этапе.
     Первый этап завершается прогоном всего словаря.
     - Второй этап.
     Это более сильная коррекция текста чем первый этап. На этом этапе отрабатываются более грубые ошибки. Восстановление пропущенных и 
     сильно искаженных букв, удаление лишних букв и т.д.
     Из словаря при помощи массива веса словарной фразы выбирается несколько десятков наиболее весомых фраз. Затем с помощью 
     классических "медленных" регулярных выражений с знаком * (произвольное значение) производится окончательная коррекция текста.     
     
    
     //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
    

    
    /*
      
    Время выполнения функции перекодировки словаря addRecords составляет примерно в 4-5 раз больше чем все остальные вместе 
    взятые функции, для данного словаря она выполнятся однократно с сохранением результатов на HD.    
    Результатом выполнения функции является:
      unsigned short  *dictionary_data;   // перекодированный массив словаря (немного меньше исходного) примерно 650мб
      unsigned int dict_size1;            // новый размер массива словаря
      unsigned int *BufE;                 // массив (int) для хранения адресов перевода каретки Enter словаря примерно 32мб
      unsigned int nEnter;                // точный размер массива адресов перевода каретки Enter словаря
      unsigned int size_BufMp=nRank*nRank;// размер зеркального массива BufMp пар букв словаря 
      
      УТОЧНИТЬ
      // массивы функции перекодировки букв словаря 
      +unsigned short *BufM;          // зеркальный массив BufM 128кб, фиксированного размера 65536*2=sizeof(unsigned short)
      +unsigned short *BufU;          // массив для восстановления исходного кода буквы (разреженного кода)
                                      // из упакованного кода буквы 128кб
      
      // массивы функции перекодировки пар букв словаря
      +unsigned short *BufMp;         // зеркальный массив пар букв BufMp словаря примерно 128-512кб (nRank*nRank)
      +unsigned short *BufUp;         // массив для восстановления исходного кода первой буквы пары    
                                      // из упакованного (компактного) кода пар букв словаря 128кб             
    */
    
    
    
    // ТРЕБОВАНИЯ К СЛОВАРЮ И ТЕКСТУ:
    // Словарь и проверяемый текст д.б. естественно в одном формате (одной кодировки)
    // Разделители (например пробелы и точки ) в словаре и тексте д.б. обязательно одинаковоми (с одинаковой кодировкой). 
    // Даже точки м.б. с разной кодировкой, поэтому if(text_data[x]==32){text_data[x]=3851;}
    // Формат сохранения словаря например в текстовом редакторе TextWrangler (MAC)
    // Unix (LF)  // Unicode (UTF-16 Little - Endian, no BOM).    
    // Фразы в словаре отделяются друг от друга переводом каретки (Enter),  "\n"
    // Текст д.б. закрыт переводом каретки (Enter), "\n" . //При корреляционном поиске переводы каретки в тексте игнорируются.
    // Первая фраза в словаре должна начинаться с пары букв которые не встречаются в основном словаре (например другого алфавита). 
    // Словарь должен содержать все (пары!) букв проверяемого текста
    // Фонт для просмотра на экране - Unicod    
    
//TIME_START
    
    
/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
    
    // dlt максимальное количество не четких пар букв в легитимной фразе словаря 
    delta=0; //0 // 2   // dlt=0; четкий поиск, dlt>0; не четкий поиск 
    
    // минимальная длина части фразы словаря (в парах букв), для корреляционного поиска, которую имеет смысл проверять как непрерывную.   
    constantPhrase=6; // 4  // 8   //// фиксированного размера = 8
    
    // сколько графики выводим на экран  // задается в table_Processing.cpp 
    //unsigned int ng=100000;   //// 100000 // 100     

/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////  

//TIME_START
    
    // Не забыть включить оптимизацию в компиляторе, с оптимизацией работает в два раза быстрее, но без пошагового отображения переменных в отладчике.
    // LLVM GCC 4.2 Code Generation пункт Optimization  Level
    
    /// отсюда стартуем для поиска по следующей странице ////
    // функция плотной упаковки разреженного кодового пространства текстовых букв.
    // осуществляет плотную паковку кодов кодов букв и пар букв проверяемого текста используя таблицы (массивы) паковки словаря.
    tableText(text);
 
 
    
    // функция создания структуры для работы с реальными координатами пар букв в распознаваемом тексте
    // выполняется один раз на весь распознаваемый текст не зависимо от размера словаря
    // если не используется lookupProcess3 и выше, функцию не запускаем.
    tableLetters();
    
    // функция применения словаря к распознаваемому тексту // word processing.
    tableProcessing(ANY_MATCH);
    
    //exit(0);
    // функция обработки результатов ассоциативного поиска 
    renderResult(text);
    
    /// функция поиска координат найденных корреляционных фраз в распознаваемом тексте по короткому словарю. Отсев корреляционных фраз. 
    /// table_TextSearch();

     
    //TIME_PRINT_
    
    
}//-------------------------------------------------------------------------------------------------------------------------------------------- 


/*
 // -------------------------------------------------------------------------------------------------------------------------------------------
 
 Адаптация этого алгоритма к OCR для быстрого, корреляционного поиска с учетом под вхождений.
 Слоги. Вместо пар букв используются слоги (в Тибетскрм это 2-3 иногда 4 буквы). 
 
 В распознанном тексте (строке или странице) для каждой позиции буквы в массиве возможных вариантов options_text запоминаем 3 
 наилучшие (возможные) коэффициента корреляции (гроздья винограда динной в три). Или иные грамматические варианты. 
 Имеется полная таблица Тибетских слогов, примерно 80000, с подсчитанными  хешами на каждый слог (наверно это 20 разрядный хеш). 
 Просматриваем тройками Тибетских слогов весь массив, сдвигая на единицу (одну букву) и учитывая 3 возможные варианта буквы, 
 там где слоги себя находят запоминаем адрес и из таблицы берем соотвествующий хеш. И используя хеш в качестве адреса 
 заносим в массив ключей key_text запомненый выше адрес. 
 В распознанном тексте один и тот же слог может встречаться более одного раза и на один адрес может претендовать более одного слога. 
 Эти адреса надо не потерять. По этому в массиве ключей нужно предусмотреть соответствующий механизм отработки. Он эквивалентен 
 механизму разрешения коллизий в хеш функциях (напремер метод цепочек).
 После просмотра всего распознанного текста массив ключей оказывается заполнен адресами всех Тибетских слогов распознанного текста.
 Теперь можно применять словарь. Словарь состоит из нумерованных и отделенных друг от друга фраз. 
 Каждая фраза состоит из 20р хешей слогов смещенных на одну букву (как и в случае распознанного текста). 
 Если в какой-то позиции себя не находит ни один слог, то на этом месте ставим ноль.
 
 Процес корреляционного поиска состоит из двух этапов
 
 - Первый этап.
 Берем первую (по порядку словаря) словарную фразу и используя хеши слогов словаря, как адрес массива ключей key_text, извлекаем 
 из этого массива адреса Тибетских слогов распознанного текста и заносим их по порядку, как восстановленный текст в массив 
 скоректированного текста proof_reading_text. Не забывая отработку коллизий в массиве ключей.
 Долее берем следующею по порядку словаря словарную фразу и аналогично пополняем массив скоректированного текста и т.д. 
 В массив скоректированного текста попадают максимально правилиные части и варианты распознанного текста (он вариантен т.к. 
 используются 3 возможные коэффициента корреляции на каждую букву и разные грамматические конструкции)
 Для каждого слога в массиве скоректированного текста рядом с ним пишется номер фразы которая его нашла в массиве ключей и ее вес.
 //Во время прогона всего словаря, в отдельном массиве weight_dictionary_phrases пишется вес словарной фразы.
 Вес словарной фразы это сколько своих слогов она смогла найти в распознанном тексте. Эти данные понадобяться на втором этапе.
 Первый этап завершается прогоном всего словаря.
 - Второй этап.
 Это более сильная коррекция текста чем первый этап. На этом этапе отрабатываются более грубые ошибки. Восстановление пропущенных
 и сильно искаженных букв, удаление лишних букв и т.д.
 Из словаря при помощи массива веса словарной фразы выбирается несколько десятков наиболее весомых фраз. 
 Затем с помощью классических "медленных" регулярных выражений с знаком * (произвольное значение) 
 производится окончательная коррекция текста.     
 
 //--------------------------------------------------------------------------------------------------------------------------------------------
 */     


//********************************************************************

// gdb /MainYagpoOCR/YagpoOCR/khenpo 


//set disassembly-flavor intel    --- Intel стиль ASM    
//set disassembly-flavor att      --- AT&T стиль ASM
//display/5i $pc -- выводить 5 линий ASM после каждого шага отладки

//break testASM  -- точка останова на функцию

//run  -- выполнить программу до точки остановки
//disassemble  -- весь листинг функции в ассемблере


//break  409   установка точки остановки на начало цикла

//continue --- выполнить до точки остановки


//s --- выполнить следующую линию кода программы

//q -- выход из gdb

//********************************************************************



/*
///// можно скомпактизировать
cout<<"Start build hash index from text"<<endl;

FILE * hFile;
string pathHash=inputData->data["root"]+HASH_PATH;
string path=pathHash;	path+="/textDictUni.txt";		cout<<" path="<<path;	
hFile = fopen(path.c_str(), "r");
unsigned long  sizeText;
if (hFile == NULL){
    cout<<"no read text dictionary. Need sortTextDictionary.txt";
    return; 
}else{
    fseek(hFile, 0, SEEK_END);
    sizeText= ftell(hFile);   cout<<endl<<" sizeText="<<sizeText<<endl;   
    fseek(hFile, 0, SEEK_SET);
    textBuffer = (char *)malloc(sizeText+1);
    if(textBuffer==NULL)exit(0);
    if (sizeText != fread(textBuffer, sizeof(char), sizeText, hFile)){
        free(textBuffer);
        exit(0); 
    }
    fclose(hFile);
}

cout<<"Done read textBuffer="<<sizeText<<endl;
/////
*/




/*
// LY Hash Function:     
// Congruential generator proposed by Leonid Yuriev. Multiplier constant suggested by M.Lavaux & F.Janssens.

// RS Hash Function:   http://vak.ru/doku. /proj/hash/sources 

// CRC32 r32, r/m* — (Подсчет CRC32)  // POPCNT r, r/m* — (Return the Count of Number of Bits Set to 1)

unsigned int x,p;
int inc,dec;
unsigned int table_size = 1 << 12;   // размер таблицы просмотра table size view
unsigned int hash;


////////*************

// построение таблиц словаря //


// Сортировка вектора.
// сортируем наш вектор от начала и до конца. http://progs.biz/cpp/stl/lessons/005.aspx
// sort(LFase.begin(), LFase.end());         // работает также как "сортировка Шейкера".
// сортируем массив от начала и до конца.
sort(dictionary_data, dictionary_data + dictionary_size);  // time=62.3142 - 670,730,860 bytes

// Сортировка массива."сортировка Шейкера". // http://www.abc-it.lv/index.php/id/610
//    Sort(dictionary_data, dictionary_size);    // очень дого >15 мин - 670,730,860 bytes
*/


//conversionDictionary(dictionary_data,dict_size, Buf1,buf_size1, Buf2,buf_size2,  &nLetterDict);  // table_Dictionary


//// функция округления числа "n" до ближайшей степени двойки в большую сторону
//void power_Two(int n);

    // блок массивов был необходим для прямой передачи данных без участия диска //
